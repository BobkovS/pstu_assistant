{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import json\n",
    "import operator\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from yargy import Parser, rule, and_\n",
    "from yargy.predicates import gram, is_capitalized, dictionary\n",
    "sys.path.insert(0, '..')\n",
    "from Common import preprocessing\n",
    "classes_map = {'DOC':0, 'ENTER':1, 'ORG':2, 'PRIV':3, 'RANG':4, 'HOST':5}\n",
    "classes_map_greet = {'QUE':0, 'GREET':1}\n",
    "\n",
    "idx_to_intent = {0:'DOC', 1:'ENTER', 2:'ORG', 3:'PRIV', 4:'RANG', 5:'HOST'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAS_GREETING = False #было ли приветствие\n",
    "#KNOW_NAME = False #знаем ли имя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..//Data//translation.csv', delimiter=';', engine='python',encoding='utf8')\n",
    "gr = pd.read_csv('..//Data//chatter.txt', delimiter=';', engine='python',encoding='utf8')\n",
    "df_ = pd.DataFrame(data={'question':df['question'],'class':df['class']})[:40]\n",
    "df_['class'] = 'QUE'\n",
    "df_ = df_.append(gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = np.array(df.question)\n",
    "questions = preprocessing.preprocess_eng_list(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность: (1342, 888)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3,ngram_range=(1,1))\n",
    "\n",
    "X = vectorizer.fit_transform(questions)\n",
    "pickle.dump(vectorizer, open(\"vectorizer\", 'wb'))\n",
    "classes = np.array(df['class'])\n",
    "y = list(map(lambda x: classes_map[x],classes))\n",
    "print(\"Размерность:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность: (88, 102)\n"
     ]
    }
   ],
   "source": [
    "greetings = np.array(df_.question)\n",
    "greetings = preprocessing.preprocess_eng_greetings_list(greetings)\n",
    "\n",
    "vectorizer_greet = TfidfVectorizer(min_df=3,ngram_range=(1,1))\n",
    "X_greet = vectorizer_greet.fit_transform(greetings)\n",
    "pickle.dump(vectorizer_greet, open(\"vectorizer_greet\", 'wb'))\n",
    "classes = np.array(df_['class'])\n",
    "y_greet = list(map(lambda x: classes_map_greet[x],classes))\n",
    "print(\"Размерность:\",X_greet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = OneVsRestClassifier(LogisticRegression(random_state=0,C=10,solver='lbfgs',)).fit(X, y)\n",
    "#pickle.dump(log_reg, open(\"log_reg\", 'wb'))\n",
    "log_reg_greet = OneVsRestClassifier(LogisticRegression(random_state=0,C=10,solver='lbfgs',)).fit(X_greet, y_greet)\n",
    "#pickle.dump(log_reg_greet, open(\"log_reg_greet\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subintent(preprocessed,intent):\n",
    "    data = None\n",
    "    with open(\"../knowledge base/en/{0}.json\".format(intent)) as f:\n",
    "        data = f.read()\n",
    "    data = json.loads(data)\n",
    "    probas = {}\n",
    "    for subintent in data:\n",
    "        count = 0\n",
    "        for keyword in data[subintent]['keywords']:\n",
    "            RULE = rule(dictionary({keyword}))\n",
    "            parser = Parser(RULE)\n",
    "            for match in parser.findall(preprocessed):\n",
    "                count = count + len(match.tokens)\n",
    "        probas[subintent] = count/len(data[subintent]['keywords'])\n",
    "    print(\"Вероятности субинтентов\",probas)\n",
    "    if any(list(probas.values())) > 0.0:\n",
    "        subintent = max(probas.items(), key=operator.itemgetter(1))[0]\n",
    "        return data[subintent]['response'][0]\n",
    "    else:\n",
    "        return fallback(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chit_chat(preprocessed):\n",
    "    preprocessed = str(preprocessed)\n",
    "    data = None\n",
    "    with open(\"../knowledge base/en/CHITCHAT.json\") as f:\n",
    "        data = f.read()\n",
    "    data = json.loads(data)\n",
    "    probas = {}\n",
    "    for subintent in data:\n",
    "        count = 0\n",
    "        for keyword in data[subintent]['keywords']:\n",
    "            RULE = rule(dictionary({keyword}))\n",
    "            parser = Parser(RULE)\n",
    "            for match in parser.findall(preprocessed):\n",
    "                count = count + len(match.tokens)\n",
    "        probas[subintent] = count/len(data[subintent]['keywords'])\n",
    "    print(\"Вероятности subgreeting\",probas)\n",
    "    if any(list(probas.values())) > 0.0:\n",
    "        subintent = max(probas.items(), key=operator.itemgetter(1))[0]\n",
    "        return data[subintent]['response'][0]\n",
    "    else:\n",
    "        return fallback(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(raw_text,WAS_GREETING):\n",
    "    preprocessed = preprocessing.preprocess_eng_greetings_list([raw_text])\n",
    "    print(\"== Продобработанный текст:\",preprocessed)\n",
    "    \n",
    "    #классифицируем это вопрос по делу или \"как дела че делаешь\"\n",
    "    v_ = vectorizer_greet.transform(preprocessed)\n",
    "    probas = log_reg_greet.predict_proba(v_)\n",
    "    print(\"== Вероятности greeting или нет\",probas[0])\n",
    "    \n",
    "    if not WAS_GREETING:\n",
    "        #классифицируем greeting или нет\n",
    "        WAS_GREETING = True\n",
    "        if probas[0][0] < probas[0][1]+0.1: #если Greeting\n",
    "            answer = chit_chat(raw_text)\n",
    "            print(\"== Ответ: \",answer)\n",
    "            os.system(\"echo \"\" \" + answer + \" \"\" | RHVoice-test\")\n",
    "            return\n",
    "    else:\n",
    "        if probas[0][0] < probas[0][1]-0.7: #если Greeting\n",
    "            answer = chit_chat(raw_text)\n",
    "            print(\"== Ответ: \",answer)\n",
    "            os.system(\"echo \"\" \" + answer + \" \"\" | RHVoice-test\")\n",
    "            return\n",
    "\n",
    "    #если по делу\n",
    "    v = vectorizer.transform(preprocessed)\n",
    "    probas = log_reg.predict_proba(v)\n",
    "    print(\"== Вероятности интентов\",probas[0])\n",
    "\n",
    "    if max(probas[0])<0.43:\n",
    "        answer = fallback(preprocessed)\n",
    "        print(\"== Ответ: \",answer)\n",
    "        os.system(\"echo \"\" \" + answer + \" \"\" | RHVoice-test\")\n",
    "    else:\n",
    "        if list(probas[0]).index(max(probas[0])) == 3: #объединяем интенты (тк некорректна выборка)\n",
    "            intent = 2\n",
    "        else:\n",
    "            intent = idx_to_intent[np.argmax(probas[0])]\n",
    "        answer = get_subintent(str(preprocessed),intent)\n",
    "        print(\"== Ответ: \",answer)\n",
    "        os.system(\"echo \"\" \" + answer + \" \"\" | RHVoice-test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fallback(text):\n",
    "    #TODO readfromfile\n",
    "    return \"I dont understand, sorry. Can you reask in different way please\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tests():\n",
    "    list_ = np.array(df.question)\n",
    "    i = random.randint(0,len(list_))\n",
    "    print(list_[i])\n",
    "    get_answer(list_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Продобработанный текст: ['What documents do i need']\n",
      "== Вероятности greeting или нет [0.46734172 0.53265828]\n",
      "== Вероятности интентов [0.86899793 0.05235625 0.03907908 0.00107282 0.00276139 0.03573254]\n",
      "Вероятности субинтентов {'ONLINE': 0.0, 'WHAT': 0.25, 'WHERE': 0.0, 'DATES': 0.1, 'HOW': 0.0, 'ISSUE': 0.0}\n",
      "== Ответ:  To apply you need following documents: passport, diploma, 2 photos, medical ref. and exam certificate.\n"
     ]
    }
   ],
   "source": [
    "WAS_GREETING = True\n",
    "get_answer(\"What documents do i need?\",WAS_GREETING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сделать все функции в один класс\n",
    "#(после приветствия спрашивать имя и тд и запоминать)\n",
    "#если несколько вопросов в одном - разделять"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
